{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecece710",
   "metadata": {},
   "source": [
    "# COLLAGE Pipeline Demo\n",
    "\n",
    "All logic lives in the `pipeline/` package. Each step follows the\n",
    "**BaseStep** contract: `ingest → transform → request → validate → end`.\n",
    "\n",
    "A single `items` list flows through the pipeline. Items are cumulative\n",
    "dicts — each step's output is merged onto the item via `to_state_dict()`.\n",
    "Downstream steps read prior outputs via `Model.from_state_dict(item)`.\n",
    "\n",
    "```\n",
    "load_products → room_recommendation → style_recommendation\n",
    "```\n",
    "\n",
    "| Step | Reads from item | Adds to item |\n",
    "|------|----------------|--------------|\n",
    "| `load_products()` | — | `Product` |\n",
    "| `RoomRecommendationStep` | `Product` | `Room` |\n",
    "| `StyleRecommendationStep` | `Product`, `Room` | `Style` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e9062f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded.\n"
     ]
    }
   ],
   "source": [
    "# My home LLM Client uses my own Anthropic API key and makes all requests in \"interactive\" mode\n",
    "# In prod, we will write an LLM Client that uses gcloud and makes batch requests instead\n",
    "# the contract of BaseLLMClient is exactly the same either way.\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    raise EnvironmentError(\n",
    "        \"ANTHROPIC_API_KEY is not set. \"\n",
    "        \"Add ANTHROPIC_API_KEY=sk-ant-... to a .env file in the repo root.\"\n",
    "    )\n",
    "\n",
    "print(\"API key loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50835ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s %(name)s — %(message)s\")\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
    "\n",
    "from pipeline import (\n",
    "    InteractiveAnthropicClient,\n",
    "    RoomRecommendationStep,\n",
    "    StyleRecommendationStep,\n",
    "    load_products,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f85377",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Build and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa867f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = InteractiveAnthropicClient(model=\"claude-3-5-haiku-20241022\", max_workers=15)\n",
    "\n",
    "# Current design is barebones, just a linear pipeline.\n",
    "# In the future, we could add branches, loops, etc. But no reason for that yet.\n",
    "graph = [\n",
    "    RoomRecommendationStep(name=\"room_rec\", llm_client=llm_client),\n",
    "    StyleRecommendationStep(name=\"style_rec\", llm_client=llm_client),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc5bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO pipeline.steps.load_products — [load_products] run_id=19c1dbc0-ea07-4de1-826f-00c8d7920b53, 5 products loaded\n",
      "INFO pipeline.steps.base_step — [room_rec] === starting ===\n",
      "INFO pipeline.steps.base_step — [room_rec] ingest: 5 items, keys: ['Product']\n",
      "INFO pipeline.steps.base_step — [room_rec] transform: 5 requests\n",
      "INFO pipeline.steps.base_step — [room_rec] request: 5 responses\n",
      "INFO pipeline.steps.base_step — [room_rec] validate: 15 outputs\n",
      "INFO pipeline.steps.base_step — [room_rec] end: 15 items, keys: ['Product', 'Room']\n",
      "INFO pipeline.steps.base_step — [room_rec] === done ===\n",
      "INFO pipeline.steps.base_step — [style_rec] === starting ===\n",
      "INFO pipeline.steps.base_step — [style_rec] ingest: 15 items, keys: ['Product', 'Room']\n",
      "INFO pipeline.steps.base_step — [style_rec] transform: 15 requests\n",
      "INFO pipeline.steps.base_step — [style_rec] request: 15 responses\n",
      "INFO pipeline.steps.base_step — [style_rec] validate: 44 outputs\n",
      "INFO pipeline.steps.base_step — [style_rec] end: 44 items, keys: ['Product', 'Room', 'Style']\n",
      "INFO pipeline.steps.base_step — [style_rec] === done ===\n"
     ]
    }
   ],
   "source": [
    "# initialize state with a list of products to use\n",
    "state = load_products()\n",
    "\n",
    "# run pipeline\n",
    "for step in graph:\n",
    "    state.update(step(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ee52e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InteractiveAnthropicClient(\n",
       "\tmodel='claude-3-5-haiku-20241022',\n",
       "\tmax_workers=15,\n",
       "\trequests=20,\n",
       "\tinput_tokens=11435,\n",
       "\toutput_tokens=8621,\n",
       "\ttotal_tokens=20056\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of LLM usage\n",
    "llm_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4xvferya0qm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL STATE\n",
      "`state` is just a dict. It has two keys:\n",
      "\t\"item_data\": 44x elements, each is: (Product, Room, Style)\n",
      "\t\"metadata\": flat dict of metadata: {'run_id': '19c1dbc0-ea07-4de1-826f-00c8d7920b53'}\n"
     ]
    }
   ],
   "source": [
    "# examine final state - this is what is passed between each stage\n",
    "from pipeline import ITEM_DATA, METADATA\n",
    "itemdata_keys = \", \".join(list(state[ITEM_DATA][0].keys()))\n",
    "\n",
    "print(\"FINAL STATE\")\n",
    "print(\"`state` is just a dict. It has two keys:\")\n",
    "print(f\"\\t\\\"{ITEM_DATA}\\\": {len(state[ITEM_DATA])}x elements, each is: ({itemdata_keys})\")\n",
    "print(f\"\\t\\\"{METADATA}\\\": flat dict of metadata: {state[METADATA]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5795ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first result:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Product': {'sku': 'OAK-TBL-001',\n",
       "  'name': 'Oakwood Dining Table',\n",
       "  'category': 'furniture',\n",
       "  'material': 'solid oak',\n",
       "  'price': 1299.0},\n",
       " 'Room': {'name': 'Dining Room',\n",
       "  'reasoning': 'The Oakwood Dining Table is perfectly suited for a dining room as it is specifically designed for dining purposes. Made of solid oak, it offers a classic, timeless look that can anchor the dining space with a high-quality, natural wood aesthetic. Its substantial material and craftsmanship justify the premium price point and make it an ideal centerpiece for formal or casual dining areas.'},\n",
       " 'Style': {'name': 'Scandinavian Minimalist',\n",
       "  'color_palette': ['white', 'light gray', 'natural wood tones'],\n",
       "  'reasoning': \"The solid oak dining table perfectly embodies the Scandinavian design philosophy of showcasing natural materials and clean lines. This style emphasizes simplicity, functionality, and the beauty of wood grain. The light, neutral color palette will highlight the oak's warm tones, creating a bright and airy dining space that feels both modern and timeless. Minimalist chairs with sleek designs and light upholstery would complement the table's natural aesthetic.\"}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine just one element. It is a unique combination of outputs from each step.\n",
    "# NOTE that our pipeline started with 5x SKUs, but\n",
    "#    - RoomRecommendationStep fanned out to 1-3x Rooms per SKU\n",
    "#    - StyleRecommendationStep fanned out to 1-3x Styles per Room\n",
    "# 5 x 3 x 3 = 45 unique results\n",
    "\n",
    "first_item = state[ITEM_DATA][0]\n",
    "print(\"first result:\")\n",
    "first_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ff0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Dining Room' reasoning='The Oakwood Dining Table is perfectly suited for a dining room as it is specifically designed for dining purposes. Made of solid oak, it offers a classic, timeless look that can anchor the dining space with a high-quality, natural wood aesthetic. Its substantial material and craftsmanship justify the premium price point and make it an ideal centerpiece for formal or casual dining areas.'\n",
      "Dining Room\n"
     ]
    }
   ],
   "source": [
    "# although state is a dict, each step accessed it in a nicely structured way - letting us do static type checking\n",
    "from pipeline.models import Room\n",
    "room_obj = Room.from_state_dict(first_item)\n",
    "print(room_obj)\n",
    "print(room_obj.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704601d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state.json: (0.048 MB)\n",
      "extended to 100k SKUs, this would be 109.029 MB\n"
     ]
    }
   ],
   "source": [
    "# state is JSON-serializable out of the box. If we want to save it to disk/gcs to checkpoint, it's trivial\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    state_path = os.path.join(tmp_dir, \"state.json\")\n",
    "    with open(state_path, \"w\") as f:\n",
    "        json.dump(state, f)\n",
    "    size = os.path.getsize(state_path)\n",
    "    size_mb = size / 1024 / 1024    \n",
    "    print(f\"state.json: ({size_mb:.3f} MB)\")\n",
    "    print(f\"extended to 100k SKUs, this would be {size_mb * 100_000 / len(state[ITEM_DATA]):.3f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6t2a4e5a9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extending the Pipeline\n",
    "\n",
    "- **Add 2x new dataclasses**: One which is the expected format of the LLM response, and one which represents a singular result, if this step fans out. The final output of the stage (`Room`) must subclass `StepOutput`\n",
    "    - For example, a `RoomResponse` contains 1-3 `Room`, and we fan out. This stage thus ~3x's the input\n",
    "- **Add a new step**: subclass `BaseStep`, implement `transform` and `validate`. These should use the dataclasses you just wrote\n",
    "    - if you dont need to use an LLM for any stage, that's fine! You can override what the `request()` method does.\n",
    "- **Fan-out in `validate`**: decapsulate the LLM response into individual `StepOutput` objects — one per unit of work for the next step. Return `(source_item, output)` pairs.\n",
    "    - Validate must discard invalid results, but you're welcome to log them somewhere if you'd like - so we can retry later\n",
    "- **Typed downstream access**: use `Model.from_state_dict(item)` in `transform` or `validate` to get typed access to any prior step's output. No need to access dict keys directly\n",
    "- **Disk serialization**: `to_state_dict()` writes `model_dump()` dicts, so state is JSON-serializable out of the box. `BaseStep.ingest()` and `BaseStep.end()` just hand off the raw dict. Users may get involved to serialize/deserialize this if they'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17acb3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
